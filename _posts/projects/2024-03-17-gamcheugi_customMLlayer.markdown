---
layout: post
title:  "Custom ML Layer for Increasing GazeTracking Precision"
date:   2024-03-17 19:15:16 +0900
categories: projects
tags: gamcheugi paper
---

최초 프로젝트에서는 오픈소스 GazeTracking 라이브러리의 동공 좌표 추적 기능을 그대로 사용한 후, 사전에 계산해놓은 동공이동-화면비(ratio)를 이용해 프레임 단위로 현재 PoG를 측정하는 방법을 사용했었다.

<a=link>오픈소스 라이브러리 분석글</>

해당 방법에는 요약하자면 크게 두 가지 단점이 있었다.

1. 결과값이 사용자의 미세한 움직임에 대처할 수 없으며 또 움직임에 지극히 민감한 점. 

2. 측정 모니터가 작은 경우 동공이 움직일 수 있는 범위 자체가 극히 한정되어서 움직임에 따른 해상도가 떨어졌다는 점. 

당시 빅프로젝트 진행 중에는 직접적인 성능을 신경 쓰지 않았던 터라 문제점 도출만 하고 대응하진 않았지만, 이제 시간이 많아졌으니 개인적으로 생각했었던 해결 방식을 하나씩 테스트해보려 한다.


먼저 화면비를 사용하는 건 오류가 발생할 가능성이 너무 많아서 폐기하는 것이 옳다고 여겼다. 따라서 PoG(x,y)를 얻기 위해 다른 방법을 생각했는데, 일단은 동공좌표(x,y)를 입력값으로 받는 ML레이어를 덧붙여서 x,y를 도출하는 함수를 fitting하는 방식을 생각해봤다.

웹캠 기반 시선추적 기술의 리뷰 논문을 읽어보았을 때는 ML레이어 방식이 성능이 좋진 않고 CNN기반이 대세인 것처럼 나와 있었는데, ensemble을 써보든, strategic overfitting을 쓰든 구체적으로 ML이 CNN보다 얼마나 성능이 낮은지 알고 싶다는 생각이 좀 들었다. 

어차피 동일한 sample extraction으로 얻은 샘플을 가지고 할 테니까. 

그리고 문득 생각했는데, 어차피 generalized model을 만들겠다는 욕망을 버린다면, 사용자마다 고유의 model을 서버에 저장해두고 calibration을 통해 overfit된 상태를 쓰면 연산 리소스가 크게 들기야 하겠지만 어느정도 정확해질 수 있지 않을까 싶었다. 또한 이 방법을 취한다면 온갖 복잡한 calibration step을 스킵할 수 있다는 점에서...나름의 편의성 향상이 있지 않을까?
나름 일종의 발상의 전?환 인데 테스트해볼 것이다. 

그래서 ML레이어, CNN레이어를 각각 테스트해볼 것이다.

2번 문제의 경우, 애초에 프로젝트 자체가 웹캠을 가진 사람을 피험자로 가정하고 있는데, 웹캠이 딸린 장비를 소유한 사람이라면 스마트폰이 없지 않을 수 없기 때문에 인풋 데이터를 2개의 채널로 나누어 1번: 웹캠으로 촬영한 정면 사진, 2번 + 스마트폰으로 촬영한 측면 사진을 놓아서 어떻게든 동공 움직임의 variance를 늘리게 위한 생각이다. 얘는 코딩하는 것 자체가 조금 힘들 것 같아서 테스트 후순위로 미뤘다. 
